{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from constants import DATA_PATH, MODELS_PATH\n",
    "from LeNet import LeNet, BATCH_SIZE\n",
    "from pruning_metadata import PruningMetadata\n",
    "from utility.pruning import (\n",
    "    calculate_parameters_amount,\n",
    "    get_parameters_to_prune,\n",
    ")\n",
    "from utility.cifar_dataset import get_dataloaders\n",
    "from dataclasses import asdict\n",
    "\n",
    "import utility\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, validation_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LeNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "early_stopper = utility.early_stopping.EarlyStopper(patience=3, min_delta=0)\n",
    "optimizer = optim.AdamW(base_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNING_VALUES = [0.20, 0.40, 0.60]\n",
    "PRUNING_NAME_TO_METHOD = {\n",
    "    \"RandomUnstructured\": prune.RandomUnstructured,\n",
    "    \"L1Unstructured\": prune.L1Unstructured,\n",
    "}\n",
    "ITER_STEPS = [1, 2, 4]\n",
    "FINETUNE_EPOCHS = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_configs = []\n",
    "\n",
    "for pruning_rate, iter_step, finetune_epochs in itertools.product(\n",
    "    PRUNING_VALUES, ITER_STEPS, FINETUNE_EPOCHS\n",
    "):\n",
    "    one_shot_configs.append(\n",
    "        PruningMetadata(\n",
    "            total_pruned=int(pruning_rate * 100),\n",
    "            pruning_step=pruning_rate,\n",
    "            finetune_epochs=(int(pruning_rate * 100) // iter_step) * finetune_epochs,\n",
    "            total_epochs=(int(pruning_rate * 100) // iter_step) * finetune_epochs,\n",
    "            method=prune.L1Unstructured,\n",
    "            early_stopping=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning rate: 0.2, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.6726, Valid Accuracy: 0.7714\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.6964, Valid Loss: 0.6990, Valid Accuracy: 0.7571\n",
      "Epoch: 1\n",
      "Train Loss: 0.6505, Valid Loss: 0.7201, Valid Accuracy: 0.7525\n",
      "Epoch: 2\n",
      "Train Loss: 0.6296, Valid Loss: 0.7416, Valid Accuracy: 0.7412\n",
      "Epoch: 3\n",
      "Train Loss: 0.6038, Valid Loss: 0.7647, Valid Accuracy: 0.7349\n",
      "Epoch: 4\n",
      "Train Loss: 0.5785, Valid Loss: 0.8027, Valid Accuracy: 0.7277\n",
      "Epoch: 5\n",
      "Train Loss: 0.5550, Valid Loss: 0.8325, Valid Accuracy: 0.7175\n",
      "Epoch: 6\n",
      "Train Loss: 0.5309, Valid Loss: 0.8190, Valid Accuracy: 0.7243\n",
      "Epoch: 7\n",
      "Train Loss: 0.5159, Valid Loss: 0.8729, Valid Accuracy: 0.7059\n",
      "Epoch: 8\n",
      "Train Loss: 0.5005, Valid Loss: 0.8717, Valid Accuracy: 0.7149\n",
      "Epoch: 9\n",
      "Train Loss: 0.4746, Valid Loss: 0.9020, Valid Accuracy: 0.7103\n",
      "Epoch: 10\n",
      "Train Loss: 0.4552, Valid Loss: 0.9444, Valid Accuracy: 0.6966\n",
      "Epoch: 11\n",
      "Train Loss: 0.4395, Valid Loss: 0.9775, Valid Accuracy: 0.6997\n",
      "Epoch: 12\n",
      "Train Loss: 0.4245, Valid Loss: 0.9626, Valid Accuracy: 0.7020\n",
      "Epoch: 13\n",
      "Train Loss: 0.4055, Valid Loss: 1.0436, Valid Accuracy: 0.6955\n",
      "Epoch: 14\n",
      "Train Loss: 0.3895, Valid Loss: 1.0702, Valid Accuracy: 0.6963\n",
      "Epoch: 15\n",
      "Train Loss: 0.3732, Valid Loss: 1.0921, Valid Accuracy: 0.6900\n",
      "Epoch: 16\n",
      "Train Loss: 0.3552, Valid Loss: 1.1210, Valid Accuracy: 0.6968\n",
      "Epoch: 17\n",
      "Train Loss: 0.3465, Valid Loss: 1.1552, Valid Accuracy: 0.6853\n",
      "Epoch: 18\n",
      "Train Loss: 0.3296, Valid Loss: 1.2065, Valid Accuracy: 0.6826\n",
      "Epoch: 19\n",
      "Train Loss: 0.3198, Valid Loss: 1.2417, Valid Accuracy: 0.6846\n",
      "Pruning rate: 0.2, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.6726, Valid Accuracy: 0.7714\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.7005, Valid Loss: 0.7115, Valid Accuracy: 0.7533\n",
      "Epoch: 1\n",
      "Train Loss: 0.6516, Valid Loss: 0.6989, Valid Accuracy: 0.7549\n",
      "Epoch: 2\n",
      "Train Loss: 0.6253, Valid Loss: 0.7409, Valid Accuracy: 0.7405\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for config in one_shot_configs:\n",
    "    pruning_rate = config.pruning_step\n",
    "    method = config.method\n",
    "    early_stopping = config.early_stopping\n",
    "    fine_tune_epochs = config.finetune_epochs\n",
    "\n",
    "    pruning_value = int(\n",
    "        round(\n",
    "            calculate_parameters_amount(get_parameters_to_prune(base_model))\n",
    "            * pruning_rate\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # load the model\n",
    "    temp_model = LeNet().to(device)\n",
    "    temp_model.load_state_dict(\n",
    "        torch.load(MODELS_PATH / Path(\"LeNet_cifar10/LeNet_cifar10.pth\"))\n",
    "    )\n",
    "    model_name = temp_model.__class__.__name__\n",
    "    model_parameters = get_parameters_to_prune(temp_model)\n",
    "\n",
    "    optimizer = optim.AdamW(temp_model.parameters())\n",
    "\n",
    "    # prune the model\n",
    "    prune.global_unstructured(\n",
    "        parameters=model_parameters,\n",
    "        pruning_method=method,\n",
    "        amount=pruning_value,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Pruning rate: {pruning_rate}, method: {method.__name__}, fine tune epochs: {fine_tune_epochs}\"\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_accuracy = utility.training.validate(\n",
    "        module=temp_model,\n",
    "        valid_dl=validation_loader,\n",
    "        loss_function=cross_entropy,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"After pruning:\\nValid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "    )\n",
    "\n",
    "    # retrain the model\n",
    "    print(\"Retraining the model\")\n",
    "    for epoch in range(fine_tune_epochs):\n",
    "        train_loss = utility.training.train_epoch(\n",
    "            module=temp_model,\n",
    "            train_dl=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        valid_loss, valid_accuracy = utility.training.validate(\n",
    "            module=temp_model,\n",
    "            valid_dl=validation_loader,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch:}\\nTrain Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    for module, name in model_parameters:\n",
    "        prune.remove(module, name)\n",
    "\n",
    "    config.method = method.__name__\n",
    "    utility.save.save_model_with_metadata(\n",
    "        model=temp_model,\n",
    "        path=f\"{MODELS_PATH}/{model_name}_pruned_{pruning_rate}_{method.__name__}\",\n",
    "        model_name=f\"{model_name}_pruned_{pruning_rate}_{method.__name__}\",\n",
    "        metadata=asdict(config),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
