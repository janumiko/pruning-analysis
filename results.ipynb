{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "from wandb.apis.public import Run as apiRun\n",
    "import json\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fixed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "entity = \"KowalskiTeam\"\n",
    "project = \"Pruning\"\n",
    "path = f\"{entity}/{project}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filters and settings\n",
    "Variables that can be changed to adjust the behavior of the queries.\n",
    "\n",
    "* job_types - names used to identify the experiments\n",
    "* only_last_checkpoints - list of schedules that should only be considered the last checkpoint\n",
    "* aggregation - which columns should be aggregated\n",
    "* columns_to_drop - columns that should be dropped\n",
    "\n",
    "It returns only the \"pruning_results\" runs, which contain the final results of the grouped runs as table artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb settings\n",
    "dataset = \"cifar100\"  # cifar10, cifar100, imagenet1k\n",
    "job_types = [\"General_17-05-2024\"]\n",
    "model = \"efficientnetv2s\"  # resnet18_cifar, efficientnetv2s, resnet18_imagenet1k\n",
    "\n",
    "# dataframe settings\n",
    "only_last_checkpoint = [\"iterative\", \"logarithmic\"]\n",
    "aggregation = {\n",
    "    \"top1_accuracy\": [\"mean\", \"std\"],\n",
    "    \"top5_accuracy\": [\"mean\", \"std\"],\n",
    "    \"total_epoch\": [\"mean\", \"std\"],\n",
    "}\n",
    "columns_to_drop = [\"repeat\", \"top5_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query filters for Wandb data, MongoDB syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filters = {\n",
    "    \"config.dataset.name\": dataset,\n",
    "    \"config.model\": model,\n",
    "    \"state\": \"finished\",\n",
    "    \"jobType\": {\"$in\": job_types},\n",
    "    \"display_name\": \"pruning_results\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downloading artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 runs\n"
     ]
    }
   ],
   "source": [
    "runs = api.runs(\n",
    "    path,\n",
    "    filters=query_filters,\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All artifacts downloaded\n"
     ]
    }
   ],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=20)\n",
    "\n",
    "\n",
    "async def download_artifact(run):\n",
    "    group = run.group\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if pathlib.Path(f\"artifacts/{group}_pruning_results:v0\").exists():\n",
    "        return\n",
    "\n",
    "    artifact = await loop.run_in_executor(\n",
    "        executor, api.artifact, f\"{run.entity}/{run.project}/{group}_pruning_results:v0\"\n",
    "    )\n",
    "    await loop.run_in_executor(executor, artifact.download)\n",
    "\n",
    "\n",
    "# Get an event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Create tasks for each run\n",
    "tasks = [download_artifact(run) for run in runs]\n",
    "\n",
    "# Wait for all tasks to complete\n",
    "await asyncio.gather(*tasks)\n",
    "print(\"All artifacts downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating CSV files with aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved constant to csv\n",
      "Saved iterative to csv\n",
      "Saved one-shot to csv\n"
     ]
    }
   ],
   "source": [
    "dataframes = defaultdict(list)\n",
    "aggregation = {key: val for key, val in aggregation.items() if key not in columns_to_drop}\n",
    "\n",
    "for run in runs:\n",
    "    run: apiRun\n",
    "    group: str = run.group\n",
    "    config: dict = run.config\n",
    "    summary: dict = run.summary\n",
    "\n",
    "    scheluder_name: str = config[\"pruning\"][\"scheduler\"][\"name\"]\n",
    "    scheduler_end: float = config[\"pruning\"][\"scheduler\"][\"end\"]\n",
    "    iterations: int = summary[\"iterations\"]\n",
    "    base_top1: float = summary.get(\"base_top1_accuracy\", None)\n",
    "\n",
    "    if base_top1 is None:\n",
    "        print(f\"Warning: base_top1 is None for '{group}'\")\n",
    "\n",
    "    # open json file, it represents wandb.Table\n",
    "    with open(f\"artifacts/{group}_pruning_results:v0/pruning_results.table.json\") as f:\n",
    "        json_dict = json.load(f)\n",
    "\n",
    "    # create dataframe from json\n",
    "    df = pd.DataFrame(json_dict[\"data\"], columns=json_dict[\"columns\"])\n",
    "\n",
    "    if \"total_epoch\" not in df.columns:\n",
    "        df[\"total_epoch\"] = None\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns_to_drop, axis=1, errors=\"ignore\")\n",
    "\n",
    "    # aggregate and round\n",
    "    df = df.groupby([\"pruned_precent\"]).agg(aggregation)\n",
    "    df = df.round(4)\n",
    "\n",
    "    # join aggregated columns\n",
    "    df.columns = df.columns.map(\"_\".join)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # get only last checkpoint for given scheduler\n",
    "    if scheluder_name in only_last_checkpoint:\n",
    "        df[\"difference\"] = abs(df[\"pruned_precent\"] - scheduler_end * 100)\n",
    "        df = df[df[\"difference\"] == df[\"difference\"].min()]\n",
    "        df = df.drop(\"difference\", axis=1)\n",
    "\n",
    "    # add run config to dataframe\n",
    "    normalized_config = pd.json_normalize(config)\n",
    "    normalized_config.insert(0, \"type\", normalized_config[\"pruning.scheduler.name\"])\n",
    "\n",
    "    def set_type(row: pd.Series) -> pd.Series:\n",
    "        if row[\"type\"] != \"manual\":\n",
    "            return row\n",
    "\n",
    "        pruning_steps = row[\"pruning.scheduler.pruning_steps\"]\n",
    "        if len(pruning_steps) > 1:\n",
    "            if pruning_steps[0][0] == pruning_steps[1][0]:\n",
    "                row[\"type\"] = \"manual_constant\"\n",
    "            else:\n",
    "                row[\"type\"] = \"manual_geometric\"\n",
    "        elif len(pruning_steps) == 1:\n",
    "            row[\"type\"] = \"manual_one_shot\"\n",
    "\n",
    "        return row\n",
    "\n",
    "    normalized_config = normalized_config.apply(set_type, axis=1)\n",
    "\n",
    "    config_series = normalized_config.squeeze()\n",
    "    for key, value in config_series.items():\n",
    "        if isinstance(value, list) or isinstance(value, dict):\n",
    "            value = json.dumps(value)\n",
    "        df[key] = value\n",
    "\n",
    "    # additional columns\n",
    "    df[\"group\"] = group\n",
    "    df[\"iterations\"] = iterations\n",
    "    df[\"base_top1\"] = base_top1\n",
    "\n",
    "    dataframes[scheluder_name].append(df)\n",
    "\n",
    "# Get the current date and time\n",
    "date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create a directory to store the csv files\n",
    "path = Path(f\"csvs/{dataset}_{date}\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save to csv\n",
    "for name in dataframes:\n",
    "    dataframes[name] = pd.concat(dataframes[name])\n",
    "\n",
    "    if \"type\" in dataframes[name].columns:\n",
    "        type_col = dataframes[name].pop(\"type\")\n",
    "        dataframes[name].insert(0, \"type\", type_col)\n",
    "\n",
    "    # sort by pruned_precent and then top1_accuracy\n",
    "    dataframes[name] = dataframes[name].sort_values(\n",
    "        by=[\"type\", \"pruned_precent\", \"top1_accuracy_mean\"], ascending=[True, True, False]\n",
    "    )\n",
    "\n",
    "    dataframes[name].to_csv(f\"{path}/pruning_results_{dataset}_{name}_{date}.csv\", index=False)\n",
    "    print(f\"Saved {name} to csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
