{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Callable, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FashionMNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# split into validation and train datasets\n",
    "train_ds = datasets.FashionMNIST(\"data\", train=True, transform=transform, download=True)\n",
    "train_ds, valid_ds = random_split(train_ds, [0.8, 0.2])\n",
    "\n",
    "test_ds = datasets.FashionMNIST(\"data\", train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1, 6, kernel_size=5, stride=1, padding=2\n",
    "        )  # 28*28->32*32-->28*28\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utilities for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int = 1, min_delta: int = 0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "\n",
    "    def early_stop(self, validation_loss: float) -> bool:\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_function: Callable,\n",
    "    epochs: int,\n",
    "    early_stopper: EarlyStopper | None = None,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[float, float]:\n",
    "    valid_loss = 0\n",
    "    valid_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X, y in train_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            train_loss = loss_function(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_dl:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = model(X)\n",
    "                valid_loss += loss_function(pred, y)\n",
    "\n",
    "                # Compute accuracy\n",
    "                valid_accuracy += (pred.argmax(1) == y).float().mean()\n",
    "\n",
    "        valid_loss /= len(valid_dl)\n",
    "        valid_accuracy /= len(valid_dl)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch #{epoch + 1}:\\t validation loss: {valid_loss:.4f}\\t validation accuracy: {valid_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        if early_stopper is not None and early_stopper.early_stop(valid_loss):\n",
    "            print(\"Early stopping\")\n",
    "            return (valid_loss, valid_accuracy)\n",
    "\n",
    "    return (valid_loss, valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to test the model\n",
    "def test(\n",
    "    model: nn.Module,\n",
    "    test_dl,\n",
    "    loss_function: Callable,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[float, float]:\n",
    "    size = len(test_dl.dataset)\n",
    "    num_batches = len(test_dl)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = (correct / size) * 100\n",
    "\n",
    "    return (test_loss, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants\n",
    "BATCH_SIZE: int = 32\n",
    "LEARNING_RATE: float = 0.01\n",
    "EPOCHS: int = 10\n",
    "MOMENTUM: float = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LeNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "optimizer = optim.SGD(base_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# create the data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubuss/miniforge3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1:\t validation loss: 0.4725\t validation accuracy: 0.8167\n",
      "Epoch #2:\t validation loss: 0.4276\t validation accuracy: 0.8394\n",
      "Epoch #3:\t validation loss: 0.3471\t validation accuracy: 0.8722\n",
      "Epoch #4:\t validation loss: 0.3434\t validation accuracy: 0.8725\n",
      "Epoch #5:\t validation loss: 0.3186\t validation accuracy: 0.8820\n",
      "Epoch #6:\t validation loss: 0.3093\t validation accuracy: 0.8837\n",
      "Epoch #7:\t validation loss: 0.2925\t validation accuracy: 0.8913\n",
      "Epoch #8:\t validation loss: 0.3004\t validation accuracy: 0.8886\n",
      "Epoch #9:\t validation loss: 0.2960\t validation accuracy: 0.8902\n",
      "Epoch #10:\t validation loss: 0.2934\t validation accuracy: 0.8943\n"
     ]
    }
   ],
   "source": [
    "valid_loss, valid_accuracy = fit(\n",
    "    base_model,\n",
    "    train_dl=train_loader,\n",
    "    valid_dl=validation_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_fn,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.312858 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, accuracy = test(\n",
    "    base_model, test_dl=test_loader, loss_function=loss_fn, device=device\n",
    ")\n",
    "print(f\"Test Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (flatten1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "1 Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "2 AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "3 Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "4 AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "5 Flatten(start_dim=1, end_dim=-1)\n",
      "6 Linear(in_features=400, out_features=120, bias=True)\n",
      "7 Linear(in_features=120, out_features=84, bias=True)\n",
      "8 Linear(in_features=84, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for i, module in enumerate(base_model.modules()):\n",
    "    print(i, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model.state_dict(), f\"models/{type(base_model).__name__}_fmnist.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_sparsity(m: nn.Module) -> float:\n",
    "    \"\"\"Get the sparsity of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to get the sparsity of\n",
    "\n",
    "    Returns:\n",
    "        float: percentage of weights that are zero\n",
    "    \"\"\"\n",
    "\n",
    "    total_weights = 0\n",
    "    total_zero_weights = 0\n",
    "    for _, module in m.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name:\n",
    "                total_weights += float(param.nelement())\n",
    "                total_zero_weights += float(torch.sum(param == 0))\n",
    "\n",
    "    sparsity = 100.0 * total_zero_weights / total_weights\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "def get_layers_sparsity(model: nn.Module) -> list[tuple[str, float]]:\n",
    "    \"\"\"Get the sparsity of each layer in the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to get the sparsity of\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, float]]: List of tuples containing the layer name and the sparsity\n",
    "    \"\"\"\n",
    "\n",
    "    layers_sparsity = []\n",
    "    for layer_name, module in model.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name:\n",
    "                layer_sparsity = (\n",
    "                    100.0 * float(torch.sum(param == 0)) / float(param.nelement())\n",
    "                )\n",
    "                layers_sparsity.append((layer_name, layer_sparsity))\n",
    "\n",
    "    return layers_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_to_prune(model: nn.Module) -> list[nn.Parameter]:\n",
    "    return [\n",
    "        (module, \"weight\")\n",
    "        for module in model.modules()\n",
    "        if isinstance(module, nn.Conv2d | nn.Linear)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNING_VALUES = [0.2, 0.4, 0.6, 0.8, 0.9, 0.95]\n",
    "PRUNING_METHODS = {\n",
    "    \"RandomUnstructured\": prune.RandomUnstructured,\n",
    "    \"L1Unstructured\": prune.L1Unstructured,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PruningResult:\n",
    "    method: str\n",
    "    pruning_rate: float\n",
    "    val_accuracy: float\n",
    "    val_loss: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.2, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.2953\t validation accuracy: 0.8905\n",
      "Epoch #2:\t validation loss: 0.2848\t validation accuracy: 0.8933\n",
      "Epoch #3:\t validation loss: 0.2930\t validation accuracy: 0.8933\n",
      "Post-pruning sparsity: 80.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.2, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.3034\t validation accuracy: 0.8901\n",
      "Epoch #2:\t validation loss: 0.2719\t validation accuracy: 0.8984\n",
      "Epoch #3:\t validation loss: 0.2677\t validation accuracy: 0.9031\n",
      "Post-pruning sparsity: 80.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.4, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.3284\t validation accuracy: 0.8778\n",
      "Epoch #2:\t validation loss: 0.3062\t validation accuracy: 0.8879\n",
      "Epoch #3:\t validation loss: 0.2904\t validation accuracy: 0.8940\n",
      "Post-pruning sparsity: 60.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.4, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.2796\t validation accuracy: 0.8975\n",
      "Epoch #2:\t validation loss: 0.2745\t validation accuracy: 0.9000\n",
      "Epoch #3:\t validation loss: 0.2748\t validation accuracy: 0.8997\n",
      "Post-pruning sparsity: 60.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.6, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.4005\t validation accuracy: 0.8511\n",
      "Epoch #2:\t validation loss: 0.3734\t validation accuracy: 0.8609\n",
      "Epoch #3:\t validation loss: 0.3279\t validation accuracy: 0.8795\n",
      "Post-pruning sparsity: 40.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.6, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.2856\t validation accuracy: 0.8950\n",
      "Epoch #2:\t validation loss: 0.2793\t validation accuracy: 0.8969\n",
      "Epoch #3:\t validation loss: 0.2892\t validation accuracy: 0.8947\n",
      "Post-pruning sparsity: 40.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.8, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.5317\t validation accuracy: 0.7964\n",
      "Epoch #2:\t validation loss: 0.4491\t validation accuracy: 0.8403\n",
      "Epoch #3:\t validation loss: 0.4139\t validation accuracy: 0.8488\n",
      "Post-pruning sparsity: 20.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.8, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.2738\t validation accuracy: 0.8993\n",
      "Epoch #2:\t validation loss: 0.2731\t validation accuracy: 0.9009\n",
      "Epoch #3:\t validation loss: 0.2748\t validation accuracy: 0.8994\n",
      "Post-pruning sparsity: 20.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.9, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.6165\t validation accuracy: 0.7657\n",
      "Epoch #2:\t validation loss: 0.5337\t validation accuracy: 0.8033\n",
      "Epoch #3:\t validation loss: 0.4735\t validation accuracy: 0.8228\n",
      "Post-pruning sparsity: 10.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.9, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.2862\t validation accuracy: 0.8949\n",
      "Epoch #2:\t validation loss: 0.2806\t validation accuracy: 0.8977\n",
      "Epoch #3:\t validation loss: 0.2914\t validation accuracy: 0.8910\n",
      "Post-pruning sparsity: 10.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.95, method: RandomUnstructured\n",
      "Epoch #1:\t validation loss: 0.9416\t validation accuracy: 0.6766\n",
      "Epoch #2:\t validation loss: 0.6308\t validation accuracy: 0.7642\n",
      "Epoch #3:\t validation loss: 0.5538\t validation accuracy: 0.7952\n",
      "Post-pruning sparsity: 5.00%\n",
      "Pre-pruning sparsity: 100.00%\n",
      "Pruning rate: 0.95, method: L1Unstructured\n",
      "Epoch #1:\t validation loss: 0.3170\t validation accuracy: 0.8848\n",
      "Epoch #2:\t validation loss: 0.3064\t validation accuracy: 0.8900\n",
      "Epoch #3:\t validation loss: 0.3025\t validation accuracy: 0.8927\n",
      "Post-pruning sparsity: 5.00%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pruning_rate in PRUNING_VALUES:\n",
    "    for method_name, method in PRUNING_METHODS.items():\n",
    "        # load the model\n",
    "        temp_model = LeNet().to(device)\n",
    "        temp_model.load_state_dict(torch.load(\"models/LeNet_fmnist.pth\"))\n",
    "        model_parameters = get_parameters_to_prune(temp_model)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(\n",
    "            temp_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM\n",
    "        )\n",
    "\n",
    "        # create the data loaders\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validation_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "        print(f\"Pre-pruning sparsity: {100 - get_model_sparsity(temp_model):.2f}%\")\n",
    "\n",
    "        # prune the model\n",
    "        prune.global_unstructured(\n",
    "            parameters=model_parameters,\n",
    "            pruning_method=method,\n",
    "            amount=pruning_rate,\n",
    "        )\n",
    "\n",
    "        print(f\"Pruning rate: {pruning_rate}, method: {method_name}\")\n",
    "\n",
    "        val_loss, val_accuracy = fit(\n",
    "            model=temp_model,\n",
    "            train_dl=train_loader,\n",
    "            valid_dl=validation_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=loss_fn,\n",
    "            epochs=3,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            PruningResult(\n",
    "                method=method_name,\n",
    "                pruning_rate=pruning_rate,\n",
    "                val_accuracy=val_accuracy,\n",
    "                val_loss=val_loss,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for module, name in model_parameters:\n",
    "            prune.remove(module, name)\n",
    "\n",
    "        print(f\"Post-pruning sparsity: {100 - get_model_sparsity(temp_model):.2f}%\")\n",
    "        torch.save(\n",
    "            temp_model.state_dict(),\n",
    "            f\"models/{type(temp_model).__name__}_pruned_{pruning_rate}_{method_name}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method\t Pruning Rate\t Validation Loss\t Validation Accuracy\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.2, val_accuracy=tensor(0.8933, device='cuda:0'), val_loss=0.29304863429069516)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.2, val_accuracy=tensor(0.9031, device='cuda:0'), val_loss=0.26767243194580076)\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.4, val_accuracy=tensor(0.8940, device='cuda:0'), val_loss=0.29040530423323313)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.4, val_accuracy=tensor(0.8997, device='cuda:0'), val_loss=0.274792817885677)\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.6, val_accuracy=tensor(0.8795, device='cuda:0'), val_loss=0.3278882878422737)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.6, val_accuracy=tensor(0.8947, device='cuda:0'), val_loss=0.28920095206797125)\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.8, val_accuracy=tensor(0.8488, device='cuda:0'), val_loss=0.41385032212734224)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.8, val_accuracy=tensor(0.8994, device='cuda:0'), val_loss=0.2748214331269264)\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.9, val_accuracy=tensor(0.8228, device='cuda:0'), val_loss=0.4735144971211751)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.9, val_accuracy=tensor(0.8910, device='cuda:0'), val_loss=0.2914292891522249)\n",
      "PruningResult(method='RandomUnstructured', pruning_rate=0.95, val_accuracy=tensor(0.7952, device='cuda:0'), val_loss=0.553762704372406)\n",
      "PruningResult(method='L1Unstructured', pruning_rate=0.95, val_accuracy=tensor(0.8927, device='cuda:0'), val_loss=0.30249639210104945)\n"
     ]
    }
   ],
   "source": [
    "print(\"Method\\t Pruning Rate\\t Validation Loss\\t Validation Accuracy\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8903\n",
      "Iteration #1:\t validation loss: 0.2889\t validation accuracy: 0.8903\n",
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8902\n",
      "Iteration #2:\t validation loss: 0.2889\t validation accuracy: 0.8902\n",
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8903\n",
      "Iteration #3:\t validation loss: 0.2889\t validation accuracy: 0.8903\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8903\n",
      "Iteration #4:\t validation loss: 0.2890\t validation accuracy: 0.8903\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8903\n",
      "Iteration #5:\t validation loss: 0.2890\t validation accuracy: 0.8903\n",
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8904\n",
      "Iteration #6:\t validation loss: 0.2889\t validation accuracy: 0.8904\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8905\n",
      "Iteration #7:\t validation loss: 0.2890\t validation accuracy: 0.8905\n",
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8906\n",
      "Iteration #8:\t validation loss: 0.2889\t validation accuracy: 0.8906\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8906\n",
      "Iteration #9:\t validation loss: 0.2890\t validation accuracy: 0.8906\n",
      "Epoch #1:\t validation loss: 0.2891\t validation accuracy: 0.8902\n",
      "Iteration #10:\t validation loss: 0.2891\t validation accuracy: 0.8902\n",
      "Epoch #1:\t validation loss: 0.2892\t validation accuracy: 0.8900\n",
      "Iteration #11:\t validation loss: 0.2892\t validation accuracy: 0.8900\n",
      "Epoch #1:\t validation loss: 0.2891\t validation accuracy: 0.8906\n",
      "Iteration #12:\t validation loss: 0.2891\t validation accuracy: 0.8906\n",
      "Epoch #1:\t validation loss: 0.2891\t validation accuracy: 0.8903\n",
      "Iteration #13:\t validation loss: 0.2891\t validation accuracy: 0.8903\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8904\n",
      "Iteration #14:\t validation loss: 0.2890\t validation accuracy: 0.8904\n",
      "Epoch #1:\t validation loss: 0.2889\t validation accuracy: 0.8906\n",
      "Iteration #15:\t validation loss: 0.2889\t validation accuracy: 0.8906\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8904\n",
      "Iteration #16:\t validation loss: 0.2890\t validation accuracy: 0.8904\n",
      "Epoch #1:\t validation loss: 0.2890\t validation accuracy: 0.8902\n",
      "Iteration #17:\t validation loss: 0.2890\t validation accuracy: 0.8902\n",
      "Epoch #1:\t validation loss: 0.2891\t validation accuracy: 0.8908\n",
      "Iteration #18:\t validation loss: 0.2891\t validation accuracy: 0.8908\n",
      "Epoch #1:\t validation loss: 0.2893\t validation accuracy: 0.8908\n",
      "Iteration #19:\t validation loss: 0.2893\t validation accuracy: 0.8908\n",
      "Epoch #1:\t validation loss: 0.2894\t validation accuracy: 0.8907\n",
      "Iteration #20:\t validation loss: 0.2894\t validation accuracy: 0.8907\n"
     ]
    }
   ],
   "source": [
    "RANGE: int = 20\n",
    "ITER_PRUNING_AMOUNT: float = 0.01\n",
    "for method_name, method in PRUNING_METHODS.items():\n",
    "    print(\n",
    "        f\"Iterative pruning using {method_name} for {RANGE} iterations with amount {ITER_PRUNING_AMOUNT}\"\n",
    "    )\n",
    "\n",
    "    iterative_model = LeNet().to(device)\n",
    "    iterative_model.load_state_dict(torch.load(\"models/LeNet_fmnist.pth\"))\n",
    "\n",
    "    iterative_model_parameters = get_parameters_to_prune(iterative_model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(temp_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "    # create the data loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    for iteration in range(RANGE):\n",
    "        prune.global_unstructured(\n",
    "            parameters=iterative_model_parameters,\n",
    "            pruning_method=method,\n",
    "            amount=ITER_PRUNING_AMOUNT,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = fit(\n",
    "            model=iterative_model,\n",
    "            train_dl=train_loader,\n",
    "            valid_dl=validation_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=loss_fn,\n",
    "            epochs=1,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Iteration #{iteration + 1}:\\t validation loss: {val_loss:.4f}\\t validation accuracy: {val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    for module, name in iterative_model_parameters:\n",
    "        prune.remove(module, name)\n",
    "\n",
    "    torch.save(\n",
    "        iterative_model.state_dict(),\n",
    "        f\"models/{type(iterative_model).__name__}_iterative_pruned_0.{RANGE}_{method_name}.pth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LeNet_pruned_0.2_L1Unstructured\n",
      "Loaded LeNet_pruned_0.8_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.2_RandomUnstructured\n",
      "Loaded LeNet_fmnist\n",
      "Loaded LeNet_pruned_0.8_L1Unstructured\n",
      "Loaded LeNet_pruned_0.4_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.9_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.95_L1Unstructured\n",
      "Loaded LeNet_pruned_0.4_L1Unstructured\n",
      "Loaded LeNet_pruned_0.6_L1Unstructured\n",
      "Loaded LeNet_pruned_0.9_L1Unstructured\n",
      "Loaded LeNet_pruned_0.95_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.6_RandomUnstructured\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for file in Path(\"models\").glob(\"*.pth\"):\n",
    "    model = LeNet().to(device)\n",
    "    temp = torch.load(file)\n",
    "    model.load_state_dict(temp)\n",
    "    print(f\"Loaded {file.stem}\")\n",
    "    models.append((file.stem, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LeNet_fmnist\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.312858 \n",
      "\n",
      "Model LeNet_pruned_0.2_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.292792 \n",
      "\n",
      "Model LeNet_pruned_0.2_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.312677 \n",
      "\n",
      "Model LeNet_pruned_0.4_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.295847 \n",
      "\n",
      "Model LeNet_pruned_0.4_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.300837 \n",
      "\n",
      "Model LeNet_pruned_0.6_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.303628 \n",
      "\n",
      "Model LeNet_pruned_0.6_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.346925 \n",
      "\n",
      "Model LeNet_pruned_0.8_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.290749 \n",
      "\n",
      "Model LeNet_pruned_0.8_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.436412 \n",
      "\n",
      "Model LeNet_pruned_0.95_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.320566 \n",
      "\n",
      "Model LeNet_pruned_0.95_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.581298 \n",
      "\n",
      "Model LeNet_pruned_0.9_L1Unstructured\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.308461 \n",
      "\n",
      "Model LeNet_pruned_0.9_RandomUnstructured\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.495840 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in sorted(models, key=lambda x: x[0]):\n",
    "    test_loss, accuracy = test(\n",
    "        model, test_dl=test_loader, loss_function=loss_fn, device=device\n",
    "    )\n",
    "    print(f\"Model {name}\")\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LeNet_fmnist\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 100.00%\n",
      "conv2: 100.00%\n",
      "fc1: 100.00%\n",
      "fc2: 100.00%\n",
      "fc3: 100.00%\n",
      "Model Sparsity: 100.0%\n",
      "Model LeNet_pruned_0.2\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 95.33%\n",
      "conv2: 89.62%\n",
      "fc1: 77.76%\n",
      "fc2: 86.80%\n",
      "fc3: 95.95%\n",
      "Model Sparsity: 80.0%\n",
      "Model LeNet_pruned_0.4\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 94.00%\n",
      "conv2: 80.50%\n",
      "fc1: 55.49%\n",
      "fc2: 73.50%\n",
      "fc3: 90.95%\n",
      "Model Sparsity: 60.0%\n",
      "Model LeNet_pruned_0.6\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 90.00%\n",
      "conv2: 69.92%\n",
      "fc1: 33.73%\n",
      "fc2: 58.30%\n",
      "fc3: 84.40%\n",
      "Model Sparsity: 40.0%\n",
      "Model LeNet_pruned_0.8\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 83.33%\n",
      "conv2: 50.75%\n",
      "fc1: 15.16%\n",
      "fc2: 30.66%\n",
      "fc3: 69.17%\n",
      "Model Sparsity: 20.0%\n",
      "Model LeNet_pruned_0.9\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 75.33%\n",
      "conv2: 35.38%\n",
      "fc1: 6.55%\n",
      "fc2: 15.65%\n",
      "fc3: 54.88%\n",
      "Model Sparsity: 10.0%\n",
      "Model LeNet_pruned_0.95\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 67.33%\n",
      "conv2: 24.29%\n",
      "fc1: 2.63%\n",
      "fc2: 7.53%\n",
      "fc3: 43.81%\n",
      "Model Sparsity: 5.000813404912961%\n",
      "Model LeNet_pruned_iterative_pruned_0.20\n",
      "##########\n",
      "Layer Sparsity:\n",
      "conv1: 96.67%\n",
      "conv2: 90.42%\n",
      "fc1: 79.80%\n",
      "fc2: 87.74%\n",
      "fc3: 96.67%\n",
      "Model Sparsity: 81.79111761835041%\n"
     ]
    }
   ],
   "source": [
    "for name, model in sorted(models, key=lambda x: x[0]):\n",
    "    print(f\"Model {name}\")\n",
    "    print(\"#\" * 10)\n",
    "\n",
    "    print(\"Layer Sparsity:\")\n",
    "    for layer_name, layer_sparsity in get_layers_sparsity(model):\n",
    "        print(f\"{layer_name}: {100 - layer_sparsity:.2f}%\")\n",
    "    print(f\"Model Sparsity: {100 - get_model_sparsity(model)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
