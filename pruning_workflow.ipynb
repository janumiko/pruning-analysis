{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "import utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH: str = \"models\"\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, name: str):\n",
    "    torch.save(model.state_dict(), f\"{MODELS_PATH}/{name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FashionMNIST data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# split into validation and train datasets\n",
    "train_ds = datasets.FashionMNIST(\"data\", train=True, transform=transform, download=True)\n",
    "train_ds, valid_ds = random_split(train_ds, [0.8, 0.2])\n",
    "\n",
    "test_ds = datasets.FashionMNIST(\"data\", train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1, 6, kernel_size=5, stride=1, padding=2\n",
    "        )  # 28*28->32*32-->28*28\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the constants\n",
    "BATCH_SIZE: int = 32\n",
    "LEARNING_RATE: float = 0.01\n",
    "EPOCHS: int = 50\n",
    "MOMENTUM: float = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LeNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "early_stopper = utility.early_stopping.EarlyStopper(patience=3, min_delta=0)\n",
    "optimizer = optim.SGD(base_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# create the data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubuss/miniforge3/envs/ml/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 0.7892, Valid Loss: 0.5948, Valid Accuracy: 0.7713\n",
      "Epoch: 1\n",
      "Train Loss: 0.4569, Valid Loss: 0.4035, Valid Accuracy: 0.8520\n",
      "Epoch: 2\n",
      "Train Loss: 0.3855, Valid Loss: 0.3962, Valid Accuracy: 0.8498\n",
      "Epoch: 3\n",
      "Train Loss: 0.3476, Valid Loss: 0.3345, Valid Accuracy: 0.8752\n",
      "Epoch: 4\n",
      "Train Loss: 0.3233, Valid Loss: 0.3173, Valid Accuracy: 0.8844\n",
      "Epoch: 5\n",
      "Train Loss: 0.3041, Valid Loss: 0.2946, Valid Accuracy: 0.8929\n",
      "Epoch: 6\n",
      "Train Loss: 0.2859, Valid Loss: 0.3123, Valid Accuracy: 0.8823\n",
      "Epoch: 7\n",
      "Train Loss: 0.2754, Valid Loss: 0.2877, Valid Accuracy: 0.8924\n",
      "Epoch: 8\n",
      "Train Loss: 0.2610, Valid Loss: 0.2758, Valid Accuracy: 0.9003\n",
      "Epoch: 9\n",
      "Train Loss: 0.2535, Valid Loss: 0.2695, Valid Accuracy: 0.8998\n",
      "Epoch: 10\n",
      "Train Loss: 0.2428, Valid Loss: 0.2680, Valid Accuracy: 0.8988\n",
      "Epoch: 11\n",
      "Train Loss: 0.2311, Valid Loss: 0.2646, Valid Accuracy: 0.8989\n",
      "Epoch: 12\n",
      "Train Loss: 0.2250, Valid Loss: 0.2664, Valid Accuracy: 0.9010\n",
      "Epoch: 13\n",
      "Train Loss: 0.2194, Valid Loss: 0.2710, Valid Accuracy: 0.9006\n",
      "Epoch: 14\n",
      "Train Loss: 0.2107, Valid Loss: 0.2660, Valid Accuracy: 0.9015\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = utility.training.train_epoch(\n",
    "        module=base_model,\n",
    "        train_dl=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=cross_entropy,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_accuracy = utility.training.validate(\n",
    "        module=base_model,\n",
    "        valid_dl=validation_loader,\n",
    "        loss_function=cross_entropy,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch:}\\nTrain Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "    )\n",
    "\n",
    "    if early_stopper.early_stop(valid_loss):\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.287590 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, accuracy = utility.training.test(\n",
    "    base_model, test_dl=test_loader, loss_function=cross_entropy, device=device\n",
    ")\n",
    "print(f\"Test Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(base_model, f\"{type(base_model).__name__}_fmnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_to_prune(model: nn.Module) -> list[nn.Parameter]:\n",
    "    return [\n",
    "        (module, \"weight\")\n",
    "        for module in model.modules()\n",
    "        if isinstance(module, nn.Conv2d | nn.Linear)\n",
    "    ]\n",
    "\n",
    "\n",
    "def calculate_total_sparsity(\n",
    "    module: nn.Module, parameters_to_prune: Iterable[tuple[nn.Module, str]]\n",
    ") -> float:\n",
    "    total_weights = 0\n",
    "    total_zero_weights = 0\n",
    "\n",
    "    pruned_parameters: set[tuple[nn.Module, str]] = set(parameters_to_prune)\n",
    "\n",
    "    for _, module in module.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if (module, param_name) not in pruned_parameters:\n",
    "                continue\n",
    "\n",
    "            if \"weight\" in param_name:\n",
    "                total_weights += float(param.nelement())\n",
    "                total_zero_weights += float(torch.sum(param == 0))\n",
    "\n",
    "    sparsity = 100.0 * total_zero_weights / total_weights\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNING_VALUES = [0.2, 0.4, 0.6, 0.8, 0.9, 0.95]\n",
    "PRUNING_METHODS = {\n",
    "    \"RandomUnstructured\": prune.RandomUnstructured,\n",
    "    \"L1Unstructured\": prune.L1Unstructured,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameters_amount(modules: Iterable[tuple[nn.Module, str]]) -> int:\n",
    "    \"\"\"Calculate the total amount of parameters in a list of modules.\n",
    "\n",
    "    Args:\n",
    "        modules (Iterable[tuple[nn.Module, str]]): List of modules and the parameter names.\n",
    "\n",
    "    Returns:\n",
    "        int: The total amount of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    total_parameters = 0\n",
    "    for module, parameter in modules:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if param_name == parameter:\n",
    "                total_parameters += param.nelement()\n",
    "\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning rate: 0.2, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.6447, Valid Accuracy: 0.7600\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2937, Valid Accuracy: 0.8907\n",
      "Early stopping\n",
      "Pruning rate: 0.2, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.2644, Valid Accuracy: 0.9019\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2647, Valid Accuracy: 0.9043\n",
      "Early stopping\n",
      "Pruning rate: 0.4, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 1.9098, Valid Accuracy: 0.3043\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2916, Valid Accuracy: 0.8930\n",
      "Early stopping\n",
      "Pruning rate: 0.4, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.2634, Valid Accuracy: 0.9014\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2584, Valid Accuracy: 0.9043\n",
      "Epoch: 1\n",
      "Train Loss: 0.2107, Valid Loss: 0.2604, Valid Accuracy: 0.9084\n",
      "Epoch: 2\n",
      "Train Loss: 0.2107, Valid Loss: 0.2581, Valid Accuracy: 0.9059\n",
      "Epoch: 3\n",
      "Train Loss: 0.2107, Valid Loss: 0.2715, Valid Accuracy: 0.9046\n",
      "Epoch: 4\n",
      "Train Loss: 0.2107, Valid Loss: 0.2646, Valid Accuracy: 0.9094\n",
      "Epoch: 5\n",
      "Train Loss: 0.2107, Valid Loss: 0.2638, Valid Accuracy: 0.9074\n",
      "Early stopping\n",
      "Pruning rate: 0.6, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 2.2484, Valid Accuracy: 0.1227\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.3540, Valid Accuracy: 0.8684\n",
      "Early stopping\n",
      "Pruning rate: 0.6, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.2679, Valid Accuracy: 0.9012\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2483, Valid Accuracy: 0.9092\n",
      "Epoch: 1\n",
      "Train Loss: 0.2107, Valid Loss: 0.2631, Valid Accuracy: 0.9081\n",
      "Epoch: 2\n",
      "Train Loss: 0.2107, Valid Loss: 0.2645, Valid Accuracy: 0.9070\n",
      "Epoch: 3\n",
      "Train Loss: 0.2107, Valid Loss: 0.2783, Valid Accuracy: 0.9064\n",
      "Early stopping\n",
      "Pruning rate: 0.8, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 2.4149, Valid Accuracy: 0.0985\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.5403, Valid Accuracy: 0.7987\n",
      "Early stopping\n",
      "Pruning rate: 0.8, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.3197, Valid Accuracy: 0.8846\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2611, Valid Accuracy: 0.9064\n",
      "Early stopping\n",
      "Pruning rate: 0.9, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 2.4088, Valid Accuracy: 0.0974\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.6382, Valid Accuracy: 0.7434\n",
      "Early stopping\n",
      "Pruning rate: 0.9, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 0.6598, Valid Accuracy: 0.7563\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.2698, Valid Accuracy: 0.9004\n",
      "Early stopping\n",
      "Pruning rate: 0.95, method: RandomUnstructured\n",
      "After pruning:\n",
      "Valid Loss: 2.3664, Valid Accuracy: 0.1044\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 2.0811, Valid Accuracy: 0.2859\n",
      "Early stopping\n",
      "Pruning rate: 0.95, method: L1Unstructured\n",
      "After pruning:\n",
      "Valid Loss: 1.5001, Valid Accuracy: 0.4827\n",
      "Retraining the model\n",
      "Epoch: 0\n",
      "Train Loss: 0.2107, Valid Loss: 0.3269, Valid Accuracy: 0.8773\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for pruning_rate in PRUNING_VALUES:\n",
    "    pruning_value = int(\n",
    "        calculate_parameters_amount(get_parameters_to_prune(base_model)) * pruning_rate\n",
    "    )\n",
    "\n",
    "    for method_name, method in PRUNING_METHODS.items():\n",
    "        # load the model\n",
    "        temp_model = LeNet().to(device)\n",
    "        temp_model.load_state_dict(torch.load(\"models/LeNet_fmnist.pth\"))\n",
    "        model_parameters = get_parameters_to_prune(temp_model)\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            temp_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM\n",
    "        )\n",
    "\n",
    "        # prune the model\n",
    "        prune.global_unstructured(\n",
    "            parameters=model_parameters,\n",
    "            pruning_method=method,\n",
    "            amount=pruning_value,\n",
    "        )\n",
    "\n",
    "        print(f\"Pruning rate: {pruning_rate}, method: {method_name}\")\n",
    "\n",
    "        valid_loss, valid_accuracy = utility.training.validate(\n",
    "            module=temp_model,\n",
    "            valid_dl=validation_loader,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"After pruning:\\nValid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # retrain the model\n",
    "        print(\"Retraining the model\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            utility.training.train_epoch(\n",
    "                module=temp_model,\n",
    "                train_dl=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                loss_function=cross_entropy,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            valid_loss, valid_accuracy = utility.training.validate(\n",
    "                module=temp_model,\n",
    "                valid_dl=validation_loader,\n",
    "                loss_function=cross_entropy,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch:}\\nTrain Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        for module, name in model_parameters:\n",
    "            prune.remove(module, name)\n",
    "\n",
    "        torch.save(\n",
    "            temp_model.state_dict(),\n",
    "            f\"models/{type(temp_model).__name__}_pruned_{pruning_rate}_{method_name}.pth\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iterative pruning using RandomUnstructured for 20 iterations with amount 614\n",
      "--------------------\n",
      "Iteration #1:\t validation loss: 0.2740\t validation accuracy: 0.8977\n",
      "Iteration #2:\t validation loss: 0.2829\t validation accuracy: 0.8932\n",
      "Iteration #3:\t validation loss: 0.2958\t validation accuracy: 0.8881\n",
      "Iteration #4:\t validation loss: 0.3381\t validation accuracy: 0.8659\n",
      "Iteration #5:\t validation loss: 0.3730\t validation accuracy: 0.8575\n",
      "Iteration #6:\t validation loss: 0.3880\t validation accuracy: 0.8532\n",
      "Iteration #7:\t validation loss: 0.4035\t validation accuracy: 0.8487\n",
      "Iteration #8:\t validation loss: 0.4053\t validation accuracy: 0.8422\n",
      "Iteration #9:\t validation loss: 0.4330\t validation accuracy: 0.8253\n",
      "Iteration #10:\t validation loss: 0.4298\t validation accuracy: 0.8262\n",
      "Iteration #11:\t validation loss: 0.4559\t validation accuracy: 0.8132\n",
      "Iteration #12:\t validation loss: 0.4981\t validation accuracy: 0.7944\n",
      "Iteration #13:\t validation loss: 0.5491\t validation accuracy: 0.7773\n",
      "Iteration #14:\t validation loss: 0.5600\t validation accuracy: 0.7708\n",
      "Iteration #15:\t validation loss: 0.5969\t validation accuracy: 0.7568\n",
      "Iteration #16:\t validation loss: 0.6812\t validation accuracy: 0.7166\n",
      "Iteration #17:\t validation loss: 0.7215\t validation accuracy: 0.6947\n",
      "Iteration #18:\t validation loss: 0.7237\t validation accuracy: 0.6947\n",
      "Iteration #19:\t validation loss: 0.7819\t validation accuracy: 0.6668\n",
      "Iteration #20:\t validation loss: 0.8492\t validation accuracy: 0.6429\n",
      "--------------------\n",
      "Iterative pruning using L1Unstructured for 20 iterations with amount 614\n",
      "--------------------\n",
      "Iteration #1:\t validation loss: 0.2660\t validation accuracy: 0.9014\n",
      "Iteration #2:\t validation loss: 0.2660\t validation accuracy: 0.9016\n",
      "Iteration #3:\t validation loss: 0.2659\t validation accuracy: 0.9014\n",
      "Iteration #4:\t validation loss: 0.2660\t validation accuracy: 0.9015\n",
      "Iteration #5:\t validation loss: 0.2659\t validation accuracy: 0.9013\n",
      "Iteration #6:\t validation loss: 0.2658\t validation accuracy: 0.9013\n",
      "Iteration #7:\t validation loss: 0.2658\t validation accuracy: 0.9013\n",
      "Iteration #8:\t validation loss: 0.2657\t validation accuracy: 0.9011\n",
      "Iteration #9:\t validation loss: 0.2658\t validation accuracy: 0.9009\n",
      "Iteration #10:\t validation loss: 0.2657\t validation accuracy: 0.9013\n",
      "Iteration #11:\t validation loss: 0.2655\t validation accuracy: 0.9010\n",
      "Iteration #12:\t validation loss: 0.2654\t validation accuracy: 0.9019\n",
      "Iteration #13:\t validation loss: 0.2652\t validation accuracy: 0.9014\n",
      "Iteration #14:\t validation loss: 0.2651\t validation accuracy: 0.9015\n",
      "Iteration #15:\t validation loss: 0.2654\t validation accuracy: 0.9015\n",
      "Iteration #16:\t validation loss: 0.2652\t validation accuracy: 0.9017\n",
      "Iteration #17:\t validation loss: 0.2648\t validation accuracy: 0.9019\n",
      "Iteration #18:\t validation loss: 0.2649\t validation accuracy: 0.9016\n",
      "Iteration #19:\t validation loss: 0.2645\t validation accuracy: 0.9023\n",
      "Iteration #20:\t validation loss: 0.2644\t validation accuracy: 0.9020\n"
     ]
    }
   ],
   "source": [
    "RANGE: int = 20\n",
    "ITER_PRUNING_RATE: float = 0.01\n",
    "\n",
    "pruning_value = int(\n",
    "    calculate_parameters_amount(get_parameters_to_prune(base_model)) * ITER_PRUNING_RATE\n",
    ")\n",
    "\n",
    "for method_name, method in PRUNING_METHODS.items():\n",
    "    print(\n",
    "        \"-\" * 20,\n",
    "        f\"Iterative pruning using {method_name} for {RANGE} iterations with amount {pruning_value}\",\n",
    "        \"-\" * 20,\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "\n",
    "    iterative_model = LeNet().to(device)\n",
    "    iterative_model.load_state_dict(torch.load(\"models/LeNet_fmnist.pth\"))\n",
    "\n",
    "    iterative_model_parameters = get_parameters_to_prune(iterative_model)\n",
    "    optimizer = optim.SGD(temp_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "    for iteration in range(RANGE):\n",
    "        prune.global_unstructured(\n",
    "            parameters=iterative_model_parameters,\n",
    "            pruning_method=method,\n",
    "            amount=pruning_value,\n",
    "        )\n",
    "\n",
    "        train_loss = utility.training.train_epoch(\n",
    "            module=iterative_model,\n",
    "            train_dl=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = utility.training.validate(\n",
    "            module=iterative_model,\n",
    "            valid_dl=validation_loader,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Iteration #{iteration + 1}:\\t validation loss: {val_loss:.4f}\\t validation accuracy: {val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "    for module, name in iterative_model_parameters:\n",
    "        prune.remove(module, name)\n",
    "\n",
    "    save_model(\n",
    "        iterative_model,\n",
    "        f\"{type(iterative_model).__name__}_iterative_pruned_0.{RANGE}_{method_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LeNet_pruned_0.2_L1Unstructured\n",
      "Loaded LeNet_pruned_0.8_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.2_RandomUnstructured\n",
      "Loaded LeNet_iterative_pruned_0.20_RandomUnstructured\n",
      "Loaded LeNet_fmnist\n",
      "Loaded LeNet_pruned_0.8_L1Unstructured\n",
      "Loaded LeNet_pruned_0.4_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.9_RandomUnstructured\n",
      "Loaded LeNet_iterative_pruned_0.20_L1Unstructured\n",
      "Loaded LeNet_pruned_0.95_L1Unstructured\n",
      "Loaded LeNet_pruned_0.4_L1Unstructured\n",
      "Loaded LeNet_pruned_0.6_L1Unstructured\n",
      "Loaded LeNet_pruned_0.9_L1Unstructured\n",
      "Loaded LeNet_pruned_0.95_RandomUnstructured\n",
      "Loaded LeNet_pruned_0.6_RandomUnstructured\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for file in Path(\"models\").glob(\"*.pth\"):\n",
    "    model = LeNet().to(device)\n",
    "    temp = torch.load(file)\n",
    "    model.load_state_dict(temp)\n",
    "    print(f\"Loaded {file.stem}\")\n",
    "    models.append((file.stem, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in sorted(models, key=lambda x: x[0]):\n",
    "    test_loss, accuracy = utility.training.test(\n",
    "        model=model, test_dl=test_loader, loss_function=cross_entropy, device=device\n",
    "    )\n",
    "    results.append((name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LeNet_fmnist                                       accuracy: 89.89%\n",
      "Model LeNet_iterative_pruned_0.20_L1Unstructured         accuracy: 89.97%\n",
      "Model LeNet_iterative_pruned_0.20_RandomUnstructured     accuracy: 63.57%\n",
      "Model LeNet_pruned_0.2_L1Unstructured                    accuracy: 89.65%\n",
      "Model LeNet_pruned_0.2_RandomUnstructured                accuracy: 88.82%\n",
      "Model LeNet_pruned_0.4_L1Unstructured                    accuracy: 90.23%\n",
      "Model LeNet_pruned_0.4_RandomUnstructured                accuracy: 88.78%\n",
      "Model LeNet_pruned_0.6_L1Unstructured                    accuracy: 89.74%\n",
      "Model LeNet_pruned_0.6_RandomUnstructured                accuracy: 86.12%\n",
      "Model LeNet_pruned_0.8_L1Unstructured                    accuracy: 89.86%\n",
      "Model LeNet_pruned_0.8_RandomUnstructured                accuracy: 79.54%\n",
      "Model LeNet_pruned_0.95_L1Unstructured                   accuracy: 87.26%\n",
      "Model LeNet_pruned_0.95_RandomUnstructured               accuracy: 27.85%\n",
      "Model LeNet_pruned_0.9_L1Unstructured                    accuracy: 89.50%\n",
      "Model LeNet_pruned_0.9_RandomUnstructured                accuracy: 74.04%\n"
     ]
    }
   ],
   "source": [
    "for name, accuracy in results:\n",
    "    print(f\"Model {name:50s} accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sparsity for LeNet_pruned_0.2_L1Unstructured\n",
      "Total sparsity: 80.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.8_RandomUnstructured\n",
      "Total sparsity: 20.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.2_RandomUnstructured\n",
      "Total sparsity: 80.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_iterative_pruned_0.20_RandomUnstructured\n",
      "Total sparsity: 80.02%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_fmnist\n",
      "Total sparsity: 100.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.8_L1Unstructured\n",
      "Total sparsity: 20.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.4_RandomUnstructured\n",
      "Total sparsity: 60.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.9_RandomUnstructured\n",
      "Total sparsity: 10.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_iterative_pruned_0.20_L1Unstructured\n",
      "Total sparsity: 80.02%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.95_L1Unstructured\n",
      "Total sparsity: 5.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.4_L1Unstructured\n",
      "Total sparsity: 60.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.6_L1Unstructured\n",
      "Total sparsity: 40.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.9_L1Unstructured\n",
      "Total sparsity: 10.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.95_RandomUnstructured\n",
      "Total sparsity: 5.00%\n",
      "--------------------\n",
      "Calculating sparsity for LeNet_pruned_0.6_RandomUnstructured\n",
      "Total sparsity: 40.00%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    print(f\"Calculating sparsity for {name}\")\n",
    "    print(\n",
    "        f\"Total sparsity: {100 - calculate_total_sparsity(model, get_parameters_to_prune(model)):.2f}%\"\n",
    "    )\n",
    "    print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
