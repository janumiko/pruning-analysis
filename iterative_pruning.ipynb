{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from constants import DATA_PATH, MODELS_PATH\n",
    "from LeNet import LeNet, BATCH_SIZE\n",
    "from pruning_metadata import PruningMetadata\n",
    "from utility.pruning import (\n",
    "    calculate_parameters_amount,\n",
    "    get_parameters_to_prune,\n",
    ")\n",
    "from utility.cifar_dataset import get_dataloaders\n",
    "from dataclasses import asdict\n",
    "\n",
    "import utility\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, validation_loader, test_loader = get_dataloaders(\n",
    "    data_path=DATA_PATH, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LeNet().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(base_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNING_VALUES = [0.20, 0.40, 0.60]\n",
    "PRUNING_NAME_TO_METHOD = {\n",
    "    \"RandomUnstructured\": prune.RandomUnstructured,\n",
    "    \"L1Unstructured\": prune.L1Unstructured,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER_PRUNING_RATES = [0.02, 0.04]\n",
    "RETRAIN_EPOCHS = [1]\n",
    "max_pruning_percent = max(PRUNING_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_pruning_configs = []\n",
    "\n",
    "for pruning_step, finetune_epochs in itertools.product(\n",
    "    ITER_PRUNING_RATES, RETRAIN_EPOCHS\n",
    "):\n",
    "    iterative_pruning_configs.append(\n",
    "        PruningMetadata(\n",
    "            total_pruned=max_pruning_percent,\n",
    "            pruning_step=pruning_step,\n",
    "            finetune_epochs=finetune_epochs,\n",
    "            total_epochs=None,  # set later\n",
    "            method=prune.L1Unstructured,\n",
    "            early_stopping=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target total pruning: 0.6, pruning_step: 0.02, method: L1Unstructured, finetune_epochs: 1\n",
      "Pre-Finetuning Validation loss: 0.6708\t validation accuracy: 0.7749\n",
      "Fine tuning the model\n",
      "Pruned 2 / 60% | Epoch #0\tvalidation loss: 0.7260\t validation accuracy: 0.7466\n",
      "Pre-Finetuning Validation loss: 0.7260\t validation accuracy: 0.7473\n",
      "Fine tuning the model\n",
      "Pruned 4 / 60% | Epoch #0\tvalidation loss: 0.7359\t validation accuracy: 0.7468\n",
      "Pre-Finetuning Validation loss: 0.7366\t validation accuracy: 0.7466\n",
      "Fine tuning the model\n",
      "Pruned 6 / 60% | Epoch #0\tvalidation loss: 0.7364\t validation accuracy: 0.7450\n",
      "Pre-Finetuning Validation loss: 0.7367\t validation accuracy: 0.7455\n",
      "Fine tuning the model\n",
      "Pruned 8 / 60% | Epoch #0\tvalidation loss: 0.7731\t validation accuracy: 0.7365\n",
      "Pre-Finetuning Validation loss: 0.7730\t validation accuracy: 0.7362\n",
      "Fine tuning the model\n",
      "Pruned 10 / 60% | Epoch #0\tvalidation loss: 0.7904\t validation accuracy: 0.7311\n",
      "Pre-Finetuning Validation loss: 0.7916\t validation accuracy: 0.7304\n",
      "Fine tuning the model\n",
      "Pruned 12 / 60% | Epoch #0\tvalidation loss: 0.8136\t validation accuracy: 0.7291\n",
      "Pre-Finetuning Validation loss: 0.8188\t validation accuracy: 0.7285\n",
      "Fine tuning the model\n",
      "Pruned 14 / 60% | Epoch #0\tvalidation loss: 0.8428\t validation accuracy: 0.7220\n",
      "Pre-Finetuning Validation loss: 0.8466\t validation accuracy: 0.7202\n",
      "Fine tuning the model\n",
      "Pruned 16 / 60% | Epoch #0\tvalidation loss: 0.9067\t validation accuracy: 0.7076\n",
      "Pre-Finetuning Validation loss: 0.9064\t validation accuracy: 0.7077\n",
      "Fine tuning the model\n",
      "Pruned 18 / 60% | Epoch #0\tvalidation loss: 0.8848\t validation accuracy: 0.7162\n",
      "Pre-Finetuning Validation loss: 0.8896\t validation accuracy: 0.7145\n",
      "Fine tuning the model\n",
      "Pruned 20 / 60% | Epoch #0\tvalidation loss: 0.9117\t validation accuracy: 0.7160\n",
      "Pre-Finetuning Validation loss: 0.9163\t validation accuracy: 0.7147\n",
      "Fine tuning the model\n",
      "Pruned 22 / 60% | Epoch #0\tvalidation loss: 0.9501\t validation accuracy: 0.7076\n",
      "Pre-Finetuning Validation loss: 0.9557\t validation accuracy: 0.7066\n",
      "Fine tuning the model\n",
      "Pruned 24 / 60% | Epoch #0\tvalidation loss: 0.9612\t validation accuracy: 0.7082\n",
      "Pre-Finetuning Validation loss: 0.9649\t validation accuracy: 0.7074\n",
      "Fine tuning the model\n",
      "Pruned 26 / 60% | Epoch #0\tvalidation loss: 1.0033\t validation accuracy: 0.7073\n",
      "Pre-Finetuning Validation loss: 1.0076\t validation accuracy: 0.7061\n",
      "Fine tuning the model\n",
      "Pruned 28 / 60% | Epoch #0\tvalidation loss: 1.0368\t validation accuracy: 0.7041\n",
      "Pre-Finetuning Validation loss: 1.0506\t validation accuracy: 0.7033\n",
      "Fine tuning the model\n",
      "Pruned 30 / 60% | Epoch #0\tvalidation loss: 1.1030\t validation accuracy: 0.7006\n",
      "Pre-Finetuning Validation loss: 1.1123\t validation accuracy: 0.6999\n",
      "Fine tuning the model\n",
      "Pruned 32 / 60% | Epoch #0\tvalidation loss: 1.1342\t validation accuracy: 0.7015\n",
      "Pre-Finetuning Validation loss: 1.1462\t validation accuracy: 0.6986\n",
      "Fine tuning the model\n",
      "Pruned 34 / 60% | Epoch #0\tvalidation loss: 1.1674\t validation accuracy: 0.6909\n",
      "Pre-Finetuning Validation loss: 1.1945\t validation accuracy: 0.6848\n",
      "Fine tuning the model\n",
      "Pruned 36 / 60% | Epoch #0\tvalidation loss: 1.1891\t validation accuracy: 0.6956\n",
      "Pre-Finetuning Validation loss: 1.2080\t validation accuracy: 0.6899\n",
      "Fine tuning the model\n",
      "Pruned 38 / 60% | Epoch #0\tvalidation loss: 1.2279\t validation accuracy: 0.6922\n",
      "Pre-Finetuning Validation loss: 1.2385\t validation accuracy: 0.6909\n",
      "Fine tuning the model\n",
      "Pruned 40 / 60% | Epoch #0\tvalidation loss: 1.2502\t validation accuracy: 0.6953\n",
      "Pre-Finetuning Validation loss: 1.2675\t validation accuracy: 0.6869\n",
      "Fine tuning the model\n",
      "Pruned 42 / 60% | Epoch #0\tvalidation loss: 1.3154\t validation accuracy: 0.6878\n",
      "Pre-Finetuning Validation loss: 1.3297\t validation accuracy: 0.6845\n",
      "Fine tuning the model\n",
      "Pruned 44 / 60% | Epoch #0\tvalidation loss: 1.3737\t validation accuracy: 0.6827\n",
      "Pre-Finetuning Validation loss: 1.4352\t validation accuracy: 0.6670\n",
      "Fine tuning the model\n",
      "Pruned 46 / 60% | Epoch #0\tvalidation loss: 1.4078\t validation accuracy: 0.6837\n",
      "Pre-Finetuning Validation loss: 1.4304\t validation accuracy: 0.6789\n",
      "Fine tuning the model\n",
      "Pruned 48 / 60% | Epoch #0\tvalidation loss: 1.4391\t validation accuracy: 0.6860\n",
      "Pre-Finetuning Validation loss: 1.4583\t validation accuracy: 0.6808\n",
      "Fine tuning the model\n",
      "Pruned 50 / 60% | Epoch #0\tvalidation loss: 1.4706\t validation accuracy: 0.6832\n",
      "Pre-Finetuning Validation loss: 1.5154\t validation accuracy: 0.6760\n",
      "Fine tuning the model\n",
      "Pruned 52 / 60% | Epoch #0\tvalidation loss: 1.5286\t validation accuracy: 0.6830\n",
      "Pre-Finetuning Validation loss: 1.5904\t validation accuracy: 0.6667\n",
      "Fine tuning the model\n",
      "Pruned 54 / 60% | Epoch #0\tvalidation loss: 1.5639\t validation accuracy: 0.6792\n",
      "Pre-Finetuning Validation loss: 1.6855\t validation accuracy: 0.6577\n",
      "Fine tuning the model\n",
      "Pruned 56 / 60% | Epoch #0\tvalidation loss: 1.6023\t validation accuracy: 0.6787\n",
      "Pre-Finetuning Validation loss: 1.8864\t validation accuracy: 0.6419\n",
      "Fine tuning the model\n",
      "Pruned 58 / 60% | Epoch #0\tvalidation loss: 1.6148\t validation accuracy: 0.6770\n",
      "Pre-Finetuning Validation loss: 1.7503\t validation accuracy: 0.6538\n",
      "Fine tuning the model\n",
      "Pruned 60 / 60% | Epoch #0\tvalidation loss: 1.6535\t validation accuracy: 0.6731\n",
      "Target total pruning: 0.6, pruning_step: 0.04, method: L1Unstructured, finetune_epochs: 1\n",
      "Pre-Finetuning Validation loss: 0.6712\t validation accuracy: 0.7751\n",
      "Fine tuning the model\n",
      "Pruned 4 / 60% | Epoch #0\tvalidation loss: 0.7404\t validation accuracy: 0.7410\n",
      "Pre-Finetuning Validation loss: 0.7422\t validation accuracy: 0.7415\n",
      "Fine tuning the model\n",
      "Pruned 8 / 60% | Epoch #0\tvalidation loss: 0.7405\t validation accuracy: 0.7407\n",
      "Pre-Finetuning Validation loss: 0.7443\t validation accuracy: 0.7400\n",
      "Fine tuning the model\n",
      "Pruned 12 / 60% | Epoch #0\tvalidation loss: 0.7697\t validation accuracy: 0.7372\n",
      "Pre-Finetuning Validation loss: 0.7772\t validation accuracy: 0.7338\n",
      "Fine tuning the model\n",
      "Pruned 16 / 60% | Epoch #0\tvalidation loss: 0.7552\t validation accuracy: 0.7407\n",
      "Pre-Finetuning Validation loss: 0.7602\t validation accuracy: 0.7414\n",
      "Fine tuning the model\n",
      "Pruned 20 / 60% | Epoch #0\tvalidation loss: 0.7781\t validation accuracy: 0.7359\n",
      "Pre-Finetuning Validation loss: 0.7960\t validation accuracy: 0.7306\n",
      "Fine tuning the model\n",
      "Pruned 24 / 60% | Epoch #0\tvalidation loss: 0.8174\t validation accuracy: 0.7259\n",
      "Pre-Finetuning Validation loss: 0.8322\t validation accuracy: 0.7234\n",
      "Fine tuning the model\n",
      "Pruned 28 / 60% | Epoch #0\tvalidation loss: 0.8319\t validation accuracy: 0.7226\n",
      "Pre-Finetuning Validation loss: 0.8466\t validation accuracy: 0.7197\n",
      "Fine tuning the model\n",
      "Pruned 32 / 60% | Epoch #0\tvalidation loss: 0.8714\t validation accuracy: 0.7173\n",
      "Pre-Finetuning Validation loss: 0.9119\t validation accuracy: 0.7058\n",
      "Fine tuning the model\n",
      "Pruned 36 / 60% | Epoch #0\tvalidation loss: 0.8917\t validation accuracy: 0.7190\n",
      "Pre-Finetuning Validation loss: 0.9256\t validation accuracy: 0.7094\n",
      "Fine tuning the model\n",
      "Pruned 40 / 60% | Epoch #0\tvalidation loss: 0.9089\t validation accuracy: 0.7131\n",
      "Pre-Finetuning Validation loss: 0.9802\t validation accuracy: 0.6917\n",
      "Fine tuning the model\n",
      "Pruned 44 / 60% | Epoch #0\tvalidation loss: 0.9499\t validation accuracy: 0.7169\n",
      "Pre-Finetuning Validation loss: 0.9907\t validation accuracy: 0.7029\n",
      "Fine tuning the model\n",
      "Pruned 48 / 60% | Epoch #0\tvalidation loss: 0.9933\t validation accuracy: 0.7068\n",
      "Pre-Finetuning Validation loss: 1.2106\t validation accuracy: 0.6589\n",
      "Fine tuning the model\n",
      "Pruned 52 / 60% | Epoch #0\tvalidation loss: 1.0216\t validation accuracy: 0.7036\n",
      "Pre-Finetuning Validation loss: 1.1464\t validation accuracy: 0.6707\n",
      "Fine tuning the model\n",
      "Pruned 56 / 60% | Epoch #0\tvalidation loss: 1.0539\t validation accuracy: 0.7011\n",
      "Pre-Finetuning Validation loss: 1.2995\t validation accuracy: 0.6484\n",
      "Fine tuning the model\n",
      "Pruned 60 / 60% | Epoch #0\tvalidation loss: 1.0588\t validation accuracy: 0.7034\n"
     ]
    }
   ],
   "source": [
    "for config in iterative_pruning_configs:\n",
    "    total_pruned = config.total_pruned\n",
    "    pruning_step = config.pruning_step\n",
    "    finetune_epochs = config.finetune_epochs\n",
    "    method = config.method\n",
    "    method_name = method.__name__\n",
    "\n",
    "    pruning_value = int(\n",
    "        round(\n",
    "            calculate_parameters_amount(get_parameters_to_prune(base_model))\n",
    "            * pruning_step\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Target total pruning: {total_pruned}, pruning_step: {pruning_step}, method: {method_name}, finetune_epochs: {finetune_epochs}\"\n",
    "    )\n",
    "\n",
    "    iterative_model = LeNet().to(device)\n",
    "    iterative_model.load_state_dict(\n",
    "        torch.load(MODELS_PATH / Path(\"LeNet_cifar10/LeNet_cifar10.pth\"))\n",
    "    )\n",
    "    model_name = iterative_model.__class__.__name__\n",
    "\n",
    "    iterative_model_parameters = get_parameters_to_prune(iterative_model)\n",
    "    iterative_optimizer = optim.AdamW(iterative_model.parameters())\n",
    "\n",
    "    target_sparsity = int(total_pruned * 100)\n",
    "    pruning_step_percent = int(pruning_step * 100)\n",
    "    total_epochs = 0\n",
    "    for pruned in range(\n",
    "        pruning_step_percent,\n",
    "        target_sparsity + pruning_step_percent,\n",
    "        pruning_step_percent,\n",
    "    ):\n",
    "        prune.global_unstructured(\n",
    "            parameters=iterative_model_parameters,\n",
    "            pruning_method=method,\n",
    "            amount=pruning_value,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = utility.training.validate(\n",
    "            module=iterative_model,\n",
    "            valid_dl=validation_loader,\n",
    "            loss_function=cross_entropy,\n",
    "            device=device,\n",
    "        )\n",
    "        print(\n",
    "            f\"Pre-Finetuning Validation loss: {val_loss:.4f}\\t validation accuracy: {val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        print(\"Fine tuning the model\")\n",
    "        total_epochs += finetune_epochs\n",
    "        for epoch in range(finetune_epochs):\n",
    "            train_loss = utility.training.train_epoch(\n",
    "                module=iterative_model,\n",
    "                train_dl=train_loader,\n",
    "                optimizer=iterative_optimizer,\n",
    "                loss_function=cross_entropy,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            val_loss, val_accuracy = utility.training.validate(\n",
    "                module=iterative_model,\n",
    "                valid_dl=validation_loader,\n",
    "                loss_function=cross_entropy,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Pruned {pruned} / {target_sparsity}% | Epoch #{epoch}\\tvalidation loss: {val_loss:.4f}\\t validation accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        if pruned in map(lambda x: int(x * 100), PRUNING_VALUES):\n",
    "            tmp = copy.deepcopy(iterative_model)\n",
    "            for module, name in get_parameters_to_prune(tmp):\n",
    "                prune.remove(module, name)\n",
    "\n",
    "            tmp_config = copy.deepcopy(config)\n",
    "            tmp_config.total_pruned = pruned / 100\n",
    "            tmp_config.method = method.__name__\n",
    "            tmp_config.total_epochs = total_epochs\n",
    "            utility.save.save_model_with_metadata(\n",
    "                model=tmp,\n",
    "                path=f\"{MODELS_PATH}/{model_name}_iterative_pruned_{pruned / 100}_step_{pruning_step}_{method_name}_epochs_{finetune_epochs}\",\n",
    "                model_name=f\"{model_name}_iterative_pruned_{pruned / 100}_step_{pruning_step}_{method_name}_epochs_{finetune_epochs}\",\n",
    "                metadata=asdict(tmp_config),\n",
    "            )\n",
    "\n",
    "    for module, name in iterative_model_parameters:\n",
    "        prune.remove(module, name)\n",
    "\n",
    "    config.method = method.__name__\n",
    "    config.total_epochs = total_epochs\n",
    "    utility.save.save_model_with_metadata(\n",
    "        model=iterative_model,\n",
    "        path=f\"{MODELS_PATH}/{model_name}_iterative_pruned_{total_pruned}_step_{pruning_step}_{method_name}_epochs_{finetune_epochs}\",\n",
    "        model_name=f\"{model_name}_iterative_pruned_{total_pruned}_step_{pruning_step}_{method_name}_epochs_{finetune_epochs}\",\n",
    "        metadata=asdict(config),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
